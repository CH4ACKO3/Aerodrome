{"expires":1743605056.883,"value":{"highlighted":"<!-- Syntax highlighted by torchlight.dev --><div class='line'><span style=\"color:#2f86d2; text-align: right; -webkit-user-select: none; user-select: none;\" class=\"line-number\">1</span><span style=\"color: #002339;\">import random\\nimport numpy as np\\nimport argparse\\nfrom copy import deepcopy\\nfrom tqdm import tqdm\\nfrom matplotlib import pyplot as plt\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\\nimport aerodrome\\n\\nclass ReplayBuffer:\\n    def __init__(self, buffer_size: int):\\n        self.buffer_size = buffer_size\\n        self.full = False\\n        self.pointer = 0\\n        self.obs = np.zeros((buffer_size, 4), dtype=np.float32)\\n        self.act = np.zeros((buffer_size, 1), dtype=np.float32)\\n        self.reward = np.zeros((buffer_size, 1), dtype=np.float32)\\n        self.next_obs = np.zeros((buffer_size, 4), dtype=np.float32)\\n        self.terminated = np.zeros((buffer_size, 1), dtype=np.float32)\\n        self.truncated = np.zeros((buffer_size, 1), dtype=np.float32)\\n\\n    def push(self, obs, act, reward, next_obs, terminated, truncated):\\n        self.obs[self.pointer] = obs\\n        self.act[self.pointer] = act\\n        self.reward[self.pointer] = reward\\n        self.next_obs[self.pointer] = next_obs\\n        self.terminated[self.pointer] = terminated\\n        self.truncated[self.pointer] = truncated\\n        self.pointer = (self.pointer + 1) % self.buffer_size\\n        if not self.full and self.pointer == 0:\\n            self.full = True\\n\\n    def sample(self, batch_size: int):\\n        if self.full:\\n            indices = np.random.randint(0, self.buffer_size, size=batch_size)\\n        else:\\n            indices = np.random.randint(0, self.pointer, size=batch_size)\\n\\n        data = {\\n            \\&quot;obs\\&quot;: self.obs[indices],\\n            \\&quot;act\\&quot;: self.act[indices],\\n            \\&quot;reward\\&quot;: self.reward[indices],\\n            \\&quot;next_obs\\&quot;: self.next_obs[indices],\\n            \\&quot;terminated\\&quot;: self.terminated[indices],\\n            \\&quot;truncated\\&quot;: self.truncated[indices]\\n        }\\n        return data\\n\\nclass QNetwork(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.network = nn.Sequential(\\n            nn.Linear(4, 256),\\n            nn.ReLU(),\\n            nn.Linear(256, 256),\\n            nn.ReLU(),\\n            nn.Linear(256, 2),\\n        )\\n\\n    def forward(self, x):\\n        return self.network(x)\\n\\ndef linear_schedule(start_e, end_e, duration, t):\\n    return start_e + (end_e - start_e) * min(t / duration, 1)\\n\\ndef main():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\&quot;--seed\\&quot;, type=int, default=0,\\n                        help=\\&quot;random seed of the experiment\\&quot;)\\n    parser.add_argument(\\&quot;--batch_size\\&quot;, type=int, default=2048,\\n                        help=\\&quot;the batch size of sample from the reply memory\\&quot;)\\n    parser.add_argument(\\&quot;--gamma\\&quot;, type=float, default=0.99,\\n                        help=\\&quot;discount factor\\&quot;)\\n    parser.add_argument(\\&quot;--tau\\&quot;, type=float, default=0.1,\\n                        help=\\&quot;the target network update rate\\&quot;)\\n    parser.add_argument(\\&quot;--target_network_frequency\\&quot;, type=int, default=1_000,\\n                        help=\\&quot;the frequency of target network update\\&quot;)\\n    parser.add_argument(\\&quot;--start_e\\&quot;, type=float, default=1.0,\\n                        help=\\&quot;starting epsilon\\&quot;)\\n    parser.add_argument(\\&quot;--end_e\\&quot;, type=float, default=0.01,\\n                        help=\\&quot;ending epsilon\\&quot;)\\n    parser.add_argument(\\&quot;--exploration_fraction\\&quot;, type=float, default=0.5,\\n                        help=\\&quot;the timesteps it takes to update the target network\\&quot;)\\n    parser.add_argument(\\&quot;--learning_rate\\&quot;, type=float, default=1e-3,\\n                        help=\\&quot;learning rate\\&quot;)\\n    parser.add_argument(\\&quot;--total_timesteps\\&quot;, type=int, default=1_000_000,\\n                        help=\\&quot;total number of timesteps\\&quot;)\\n    parser.add_argument(\\&quot;--buffer_size\\&quot;, type=int, default=100_000,\\n                        help=\\&quot;buffer size\\&quot;)\\n    parser.add_argument(\\&quot;--learning_starts\\&quot;, type=int, default=10_000,\\n                        help=\\&quot;timestep to start learning\\&quot;)\\n    parser.add_argument(\\&quot;--train_frequency\\&quot;, type=int, default=10,\\n                        help=\\&quot;the frequency of training\\&quot;)\\n    parser.add_argument(\\&quot;--device\\&quot;, type=str, default=\\&quot;cuda\\&quot; if torch.cuda.is_available() else \\&quot;cpu\\&quot;,\\n                        help=\\&quot;Device to run the experiment on\\&quot;)\\n    args = parser.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n    torch.backends.cudnn.deterministic = True\\n\\n    device = torch.device(args.device)\\n    print(f\\&quot;Using device: {device}\\&quot;)\\n\\n    q_network = QNetwork().to(device)\\n    target_network = QNetwork().to(device)\\n    target_network.load_state_dict(q_network.state_dict())\\n    target_network.eval()\\n\\n    optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\\n    cos_annealing_scheduler = CosineAnnealingLR(optimizer, T_max=args.total_timesteps, eta_min=args.learning_rate * 0.01)\\n\\n    replay_buffer = ReplayBuffer(args.buffer_size)\\n\\n    env = aerodrome.make(\\&quot;cartpole-v0\\&quot;)\\n    obs, info = env.reset()\\n    obs = np.array(obs, dtype=np.float32)\\n    print(obs, info)\\n\\n    episode_length = []\\n    current_step = 0\\n    for step in tqdm(range(args.total_timesteps)):\\n        current_step += 1\\n        epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, step)\\n        if random.random() &lt; epsilon:\\n            action = random.randint(0, 1)\\n        else:\\n            with torch.no_grad():\\n                q_values = q_network(torch.from_numpy(obs).unsqueeze(0).to(device))\\n                action = torch.argmax(q_values, dim=1).cpu().numpy().item()\\n\\n        next_obs, reward, terminated, truncated, info = env.step(action)\\n        next_obs = np.array(next_obs, dtype=np.float32)\\n\\n        real_next_obs = deepcopy(next_obs)\\n        replay_buffer.push(obs, action, reward, real_next_obs, terminated, truncated)\\n\\n        obs = next_obs\\n\\n        if step &gt; args.learning_starts:\\n            if step % args.train_frequency == 0:\\n                batch = replay_buffer.sample(args.batch_size)\\n                batch[\\&quot;obs\\&quot;] = torch.from_numpy(batch[\\&quot;obs\\&quot;]).to(device)\\n                batch[\\&quot;act\\&quot;] = torch.from_numpy(batch[\\&quot;act\\&quot;]).to(device)\\n                batch[\\&quot;reward\\&quot;] = torch.from_numpy(batch[\\&quot;reward\\&quot;]).to(device)\\n                batch[\\&quot;next_obs\\&quot;] = torch.from_numpy(batch[\\&quot;next_obs\\&quot;]).to(device)\\n                batch[\\&quot;terminated\\&quot;] = torch.from_numpy(batch[\\&quot;terminated\\&quot;]).to(device)\\n                batch[\\&quot;truncated\\&quot;] = torch.from_numpy(batch[\\&quot;truncated\\&quot;]).to(device)\\n\\n                with torch.no_grad():\\n                    target_max, _ = target_network(batch[\\&quot;next_obs\\&quot;]).max(dim=1, keepdim=True)\\n                    td_target = batch[\\&quot;reward\\&quot;] + args.gamma * (1 - batch[\\&quot;terminated\\&quot;]) * target_max - 10.0 * batch[\\&quot;terminated\\&quot;]\\n\\n                old_val = q_network(batch[\\&quot;obs\\&quot;])\\n                old_val = old_val.gather(1, batch[\\&quot;act\\&quot;].long())\\n\\n                loss = F.mse_loss(td_target, old_val)\\n\\n                optimizer.zero_grad()\\n                loss.backward()\\n                optimizer.step()\\n\\n            if step % args.target_network_frequency == 0:\\n                for target_param, param in zip(target_network.parameters(), q_network.parameters()):\\n                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\\n\\n        cos_annealing_scheduler.step()\\n\\n        if terminated or truncated:\\n            obs, info = env.reset()\\n            obs = np.array(obs, dtype=np.float32)\\n            episode_length.append((step, current_step))\\n            current_step = 0\\n\\n    episode_length = np.array(episode_length)\\n    fig, ax = plt.subplots()\\n    ax.plot(episode_length[:, 0], episode_length[:, 1], alpha=0.5, c=\\&quot;skyblue\\&quot;)\\n    moving_average = np.convolve(episode_length[:, 1], np.ones(100) / 100, mode=&#39;valid&#39;)\\n    ax.plot(episode_length[99:, 0], moving_average, c=\\&quot;royalblue\\&quot;)\\n    ax.set_xlabel(\\&quot;steps\\&quot;)\\n    ax.set_ylabel(\\&quot;cumulative reward\\&quot;)\\n    plt.show()\\n\\nif __name__ == \\&quot;__main__\\&quot;:\\n    main()</span></div>","classes":"torchlight","styles":"background-color: #FFFFFF; --theme-selection-background: #4373c2;"}}
