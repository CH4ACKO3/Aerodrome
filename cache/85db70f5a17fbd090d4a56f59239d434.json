{"expires":1743604837.337,"value":{"highlighted":"<!-- Syntax highlighted by torchlight.dev --><div class='line'><span style=\"color:#1b1f234d; text-align: right; -webkit-user-select: none; user-select: none;\" class=\"line-number\">1</span><span style=\"color: #24292e;\">def main():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\&quot;--seed\\&quot;, type=int, default=0,\\n                        help=\\&quot;random seed of the experiment\\&quot;)\\n    parser.add_argument(\\&quot;--num_steps\\&quot;, type=int, default=1024,\\n                        help=\\&quot;the number of steps to run per policy rollout\\&quot;)\\n    parser.add_argument(\\&quot;--total_timesteps\\&quot;, type=int, default=300_000,\\n                        help=\\&quot;the number of iterations\\&quot;)\\n    parser.add_argument(\\&quot;--batch_size\\&quot;, type=int, default=256,\\n                        help=\\&quot;the batch size of sample from the replay memory\\&quot;)\\n    parser.add_argument(\\&quot;--gamma\\&quot;, type=float, default=0.99,\\n                        help=\\&quot;discount factor\\&quot;)\\n    parser.add_argument(\\&quot;--gae_lambda\\&quot;, type=float, default=0.95,\\n                        help=\\&quot;lambda for the general advantage estimation\\&quot;)\\n    parser.add_argument(\\&quot;--num_minibatches\\&quot;, type=int, default=1,\\n                        help=\\&quot;the number of mini-batches\\&quot;)\\n    parser.add_argument(\\&quot;--update_epochs\\&quot;, type=int, default=20,\\n                        help=\\&quot;the K epochs to update the policy\\&quot;)\\n    parser.add_argument(\\&quot;--norm_adv\\&quot;, type=bool, default=True,\\n                        help=\\&quot;Toggles advantages normalization\\&quot;)\\n    parser.add_argument(\\&quot;--clip_coef\\&quot;, type=float, default=0.2,\\n                        help=\\&quot;the surrogate clipping coefficient\\&quot;)\\n    parser.add_argument(\\&quot;--clip_vloss\\&quot;, type=bool, default=True,\\n                        help=\\&quot;Toggles whether or not to use a clipped loss for the value function\\&quot;)\\n    parser.add_argument(\\&quot;--ent_coef\\&quot;, type=float, default=0.2,\\n                        help=\\&quot;coefficient of the entropy\\&quot;)\\n    parser.add_argument(\\&quot;--vf_coef\\&quot;, type=float, default=2.,\\n                        help=\\&quot;coefficient of the value function\\&quot;)\\n    parser.add_argument(\\&quot;--max_grad_norm\\&quot;, type=float, default=0.5,\\n                        help=\\&quot;the maximum norm for the gradient clipping\\&quot;)\\n    parser.add_argument(\\&quot;--target_kl\\&quot;, type=float, default=None,\\n                        help=\\&quot;the target KL divergence threshold\\&quot;)\\n\\n    parser.add_argument(\\&quot;--learning_rate\\&quot;, type=float, default=1e-4,\\n                        help=\\&quot;learning rate\\&quot;)\\n    parser.add_argument(\\&quot;--device\\&quot;, type=str, default=\\&quot;cuda\\&quot; if torch.cuda.is_available() else \\&quot;cpu\\&quot;,\\n                        help=\\&quot;Device to run the experiment on\\&quot;)\\n    args = parser.parse_args()\\n\\n    args.batch_size = args.num_steps\\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\\n    args.num_iterations = args.total_timesteps // args.batch_size\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n    torch.backends.cudnn.deterministic = True\\n\\n    device = torch.device(args.device)\\n    print(f\\&quot;Using device: {device}\\&quot;)\\n\\n    env = aerodrome.make(\\&quot;wingedcone-v0\\&quot;)\\n    object_dict = {\\n        \\&quot;name\\&quot;: \\&quot;test\\&quot;,\\n        \\&quot;integrator\\&quot;: \\&quot;rk45\\&quot;,\\n        \\&quot;S\\&quot;: 3603.0,\\n        \\&quot;c\\&quot;: 80.0,\\n        \\&quot;m\\&quot;: 9375.0,\\n\\n        \\&quot;pos\\&quot;: [0.0, 33528.0, 0.0],\\n        \\&quot;vel\\&quot;: [4590.29, 0.0, 0.0],\\n        \\&quot;ang_vel\\&quot;: [0.0, 0.0, 0.0],\\n        \\&quot;J\\&quot;: [1.0, 7*10**6, 7*10**6],\\n        \\&quot;theta\\&quot;: 0.00/180*pi,\\n        \\&quot;phi\\&quot;: 0.0,\\n        \\&quot;gamma\\&quot;: 0.0,   \\n        \\&quot;theta_v\\&quot;: 0.0,\\n        \\&quot;phi_v\\&quot;: 0.0,\\n        \\&quot;gamma_v\\&quot;: 0.0,\\n        \\&quot;alpha\\&quot;: 0.00/180*pi,\\n        \\&quot;beta\\&quot;: 0.0,\\n\\n        \\&quot;Kiz\\&quot;: 0.2597,\\n        \\&quot;Kwz\\&quot;: 1.6,\\n        \\&quot;Kaz\\&quot;: 13/2,\\n        \\&quot;Kpz\\&quot;: 0.14,\\n        \\&quot;Kp_V\\&quot;: 5.0,\\n        \\&quot;Ki_V\\&quot;: 1.0,\\n        \\&quot;Kd_V\\&quot;: 0.3\\n    }\\n\\n    object = WingedCone2D_RL(object_dict)\\n    env.add_object(object)\\n\\n    agent = Agent().to(device)\\n    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\\n    cos_annealing_scheduler = CosineAnnealingLR(optimizer, T_max=args.num_iterations, eta_min=args.learning_rate/10)\\n\\n    # Storage setup\\n    obs = torch.zeros((args.num_steps, 3)).to(device)\\n    actions = torch.zeros((args.num_steps, 1)).to(device)\\n    logprobs = torch.zeros((args.num_steps, 1)).to(device)\\n    rewards = torch.zeros((args.num_steps, 1)).to(device)\\n    dones = torch.zeros((args.num_steps, 1)).to(device)\\n    values = torch.zeros((args.num_steps, 1)).to(device)\\n\\n    global_step = 0\\n\\n    records = {\\n        \\&quot;reward\\&quot;: np.zeros(args.num_iterations),\\n        \\&quot;learning_rate\\&quot;: np.zeros(args.num_iterations),\\n        \\&quot;value_loss\\&quot;: np.zeros(args.num_iterations),\\n        \\&quot;policy_loss\\&quot;: np.zeros(args.num_iterations),\\n        \\&quot;entropy\\&quot;: np.zeros(args.num_iterations),\\n    }\\n\\n    for iteration in tqdm(range(1, args.num_iterations + 1)):\\n        env = aerodrome.make(\\&quot;wingedcone-v0\\&quot;)\\n        object = WingedCone2D_RL(object_dict)\\n        env.add_object(object)\\n\\n        next_obs, info = env.reset()\\n        next_obs = torch.Tensor(next_obs).reshape((1, -1)).to(device)\\n        first_rollout = True\\n        rollout_return = 0.0\\n        for step in range(0, args.num_steps):\\n            global_step += 1\\n            obs[step] = next_obs\\n\\n            # ALGO LOGIC: action logic\\n            with torch.no_grad():\\n                action, logprob, _, value = agent.get_action_and_value(next_obs)\\n                values[step] = value.flatten()\\n            actions[step] = action\\n            logprobs[step] = logprob\\n\\n            # TRY NOT TO MODIFY: execute the game and log data.\\n            step_action = {\\n                \\&quot;test\\&quot;: {\\&quot;Nyc\\&quot;:1.0, \\&quot;Vc\\&quot;:4590.29, \\&quot;nn_control\\&quot;:action.item()},\\n            }\\n            next_obs, reward, terminations, truncations, infos = env.step(step_action)\\n            next_done = np.logical_or(terminations, truncations)\\n            rewards[step] = torch.tensor(reward).to(device).view(-1)\\n            next_obs, next_done = torch.Tensor(next_obs).reshape((1, -1)).to(device), torch.Tensor(next_done).reshape((1, -1)).to(device)\\n\\n            dones[step] = next_done\\n\\n            if first_rollout:\\n                rollout_return += reward\\n                if next_done or step == args.num_steps - 1:\\n                    first_rollout = False\\n                    rewards[step] = rollout_return\\n\\n            if next_done:\\n                env = aerodrome.make(\\&quot;wingedcone-v0\\&quot;)\\n                object = WingedCone2D_RL(object_dict)\\n                env.add_object(object)\\n\\n                next_obs, info = env.reset()\\n                next_obs = torch.Tensor(next_obs).to(device)\\n\\n        # bootstrap value if not done\\n        with torch.no_grad():\\n            next_value = agent.get_value(next_obs).reshape(1, -1)\\n            advantages = torch.zeros_like(rewards).to(device)\\n            lastgaelam = 0\\n            for t in reversed(range(args.num_steps)):\\n                if t == args.num_steps - 1:\\n                    nextnonterminal = 1.0 - next_done\\n                    nextvalues = next_value\\n                else:\\n                    nextnonterminal = 1.0 - dones[t]\\n                    nextvalues = values[t + 1]\\n                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\\n                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\\n            returns = advantages + values\\n\\n        # flatten the batch\\n        b_obs = obs.reshape((-1, 3))\\n        b_logprobs = logprobs.reshape(-1)\\n        b_actions = actions.reshape((-1, 1))\\n        b_advantages = advantages.reshape(-1)\\n        b_returns = returns.reshape(-1)\\n        b_values = values.reshape(-1)\\n\\n        # Optimizing the policy and value network\\n        b_inds = np.arange(args.batch_size)\\n        clipfracs = []\\n        for epoch in range(args.update_epochs):\\n            np.random.shuffle(b_inds)\\n            for start in range(0, args.batch_size, args.minibatch_size):\\n                end = start + args.minibatch_size\\n                mb_inds = b_inds[start:end]\\n\\n                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\\n                logratio = newlogprob - b_logprobs[mb_inds]\\n                ratio = logratio.exp()\\n\\n                with torch.no_grad():\\n                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\\n                    old_approx_kl = (-logratio).mean()\\n                    approx_kl = ((ratio - 1) - logratio).mean()\\n                    clipfracs += [((ratio - 1.0).abs() &gt; args.clip_coef).float().mean().item()]\\n\\n                mb_advantages = b_advantages[mb_inds]\\n                if args.norm_adv:\\n                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\\n\\n                # Policy loss\\n                pg_loss1 = -mb_advantages * ratio\\n                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\\n                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\\n\\n                # Value loss\\n                newvalue = newvalue.view(-1)\\n                if args.clip_vloss:\\n                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\\n                    v_clipped = b_values[mb_inds] + torch.clamp(\\n                        newvalue - b_values[mb_inds],\\n                        -args.clip_coef,\\n                        args.clip_coef,\\n                    )\\n                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\\n                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\\n                    v_loss = 0.5 * v_loss_max.mean()\\n                else:\\n                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\\n\\n                entropy_loss = entropy.mean()\\n                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\\n\\n                optimizer.zero_grad()\\n                loss.backward()\\n                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\\n                optimizer.step()\\n\\n            if args.target_kl is not None and approx_kl &gt; args.target_kl:\\n                break\\n\\n        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\\n        var_y = np.var(y_true)\\n        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\\n\\n        records[\\&quot;reward\\&quot;][iteration - 1] = rewards.sum().item()\\n        records[\\&quot;learning_rate\\&quot;][iteration - 1] = optimizer.param_groups[0][\\&quot;lr\\&quot;]\\n        records[\\&quot;value_loss\\&quot;][iteration - 1] = v_loss.item()\\n        records[\\&quot;policy_loss\\&quot;][iteration - 1] = pg_loss.item()\\n        records[\\&quot;entropy\\&quot;][iteration - 1] = entropy_loss.item()\\n\\n        cos_annealing_scheduler.step()\\n\\n        if iteration == args.num_iterations:\\n            states = []\\n            rewards = []\\n            env = aerodrome.make(\\&quot;wingedcone-v0\\&quot;)\\n            object = WingedCone2D_RL(object_dict)\\n            env.add_object(object)\\n            next_obs, info = env.reset()\\n            next_obs = torch.Tensor(next_obs).reshape((1, -1)).to(device)\\n            step = 0\\n            while True:\\n                step += 1\\n                # ALGO LOGIC: action logic\\n                states.append(env.get_state())\\n                with torch.no_grad():\\n                    action, logprob, _, value = agent.get_action_and_value(next_obs, evaluate=True)\\n\\n                # TRY NOT TO MODIFY: execute the game and log data.\\n                step_action = {\\n                    \\&quot;test\\&quot;: {\\&quot;Nyc\\&quot;:1.0, \\&quot;Vc\\&quot;:4590.29, \\&quot;nn_control\\&quot;:action.item()},\\n                }\\n                next_obs, reward, terminations, truncations, infos = env.step(step_action)\\n                next_done = np.logical_or(terminations, truncations)\\n                next_obs = torch.Tensor(next_obs).reshape((1, -1)).to(device)\\n                rewards.append(reward)\\n                if next_done or step &gt;= 1000:\\n                    break\\n            fig, ax = plt.subplots(1, 1)\\n            x = np.array([states[i][\\&quot;pos\\&quot;][0] for i in range(len(states))])\\n            y = np.array([states[i][\\&quot;Ny\\&quot;] for i in range(len(states))])\\n            ax.plot(x, y)\\n            ax.plot(x, rewards)\\n            plt.show()\\n\\n    fig, ax = plt.subplots(1, 1)\\n    ax.plot(records[\\&quot;reward\\&quot;])\\n    ax.set_xlabel(\\&quot;Iteration\\&quot;)\\n    ax.set_ylabel(\\&quot;Reward\\&quot;)\\n    plt.show()\\n\\n    torch.save(agent.state_dict(), \\&quot;models/wingedcone_ppo.pth\\&quot;)\\n\\nif __name__ == \\&quot;__main__\\&quot;:\\n    main()</span></div>","classes":"torchlight","styles":"background-color: #fff; --theme-selection-background: #e2e5e9;"}}
