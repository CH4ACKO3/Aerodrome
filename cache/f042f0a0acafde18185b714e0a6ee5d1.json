{"expires":1743603300.767,"value":{"highlighted":"<!-- Syntax highlighted by torchlight.dev --><div class='line'><span style=\"color: #A6ACCD;\">import random</span></div><div class='line'><span style=\"color: #A6ACCD;\">import numpy as np</span></div><div class='line'><span style=\"color: #A6ACCD;\">import argparse</span></div><div class='line'><span style=\"color: #A6ACCD;\">from copy import deepcopy</span></div><div class='line'><span style=\"color: #A6ACCD;\">from tqdm import tqdm</span></div><div class='line'><span style=\"color: #A6ACCD;\">from matplotlib import pyplot as plt</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">import torch</span></div><div class='line'><span style=\"color: #A6ACCD;\">import torch.nn as nn</span></div><div class='line'><span style=\"color: #A6ACCD;\">import torch.nn.functional as F</span></div><div class='line'><span style=\"color: #A6ACCD;\">import torch.optim as optim</span></div><div class='line'><span style=\"color: #A6ACCD;\">from torch.optim.lr_scheduler import CosineAnnealingLR</span></div><div class='line'><span style=\"color: #A6ACCD;\">import aerodrome</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">class ReplayBuffer:</span></div><div class='line'><span style=\"color: #A6ACCD;\">    def __init__(self, buffer_size: int):</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.buffer_size = buffer_size</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.full = False</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.pointer = 0</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.obs = np.zeros((buffer_size, 4), dtype=np.float32)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.act = np.zeros((buffer_size, 1), dtype=np.float32)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.reward = np.zeros((buffer_size, 1), dtype=np.float32)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.next_obs = np.zeros((buffer_size, 4), dtype=np.float32)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.terminated = np.zeros((buffer_size, 1), dtype=np.float32)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.truncated = np.zeros((buffer_size, 1), dtype=np.float32)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    def push(self, obs, act, reward, next_obs, terminated, truncated):</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.obs[self.pointer] = obs</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.act[self.pointer] = act</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.reward[self.pointer] = reward</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.next_obs[self.pointer] = next_obs</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.terminated[self.pointer] = terminated</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.truncated[self.pointer] = truncated</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.pointer = (self.pointer + 1) % self.buffer_size</span></div><div class='line'><span style=\"color: #A6ACCD;\">        if not self.full and self.pointer == 0:</span></div><div class='line'><span style=\"color: #A6ACCD;\">            self.full = True</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    def sample(self, batch_size: int):</span></div><div class='line'><span style=\"color: #A6ACCD;\">        if self.full:</span></div><div class='line'><span style=\"color: #A6ACCD;\">            indices = np.random.randint(0, self.buffer_size, size=batch_size)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        else:</span></div><div class='line'><span style=\"color: #A6ACCD;\">            indices = np.random.randint(0, self.pointer, size=batch_size)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">        data = {</span></div><div class='line'><span style=\"color: #A6ACCD;\">            &quot;obs&quot;: self.obs[indices],</span></div><div class='line'><span style=\"color: #A6ACCD;\">            &quot;act&quot;: self.act[indices],</span></div><div class='line'><span style=\"color: #A6ACCD;\">            &quot;reward&quot;: self.reward[indices],</span></div><div class='line'><span style=\"color: #A6ACCD;\">            &quot;next_obs&quot;: self.next_obs[indices],</span></div><div class='line'><span style=\"color: #A6ACCD;\">            &quot;terminated&quot;: self.terminated[indices],</span></div><div class='line'><span style=\"color: #A6ACCD;\">            &quot;truncated&quot;: self.truncated[indices]</span></div><div class='line'><span style=\"color: #A6ACCD;\">        }</span></div><div class='line'><span style=\"color: #A6ACCD;\">        return data</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">class QNetwork(nn.Module):</span></div><div class='line'><span style=\"color: #A6ACCD;\">    def __init__(self):</span></div><div class='line'><span style=\"color: #A6ACCD;\">        super().__init__()</span></div><div class='line'><span style=\"color: #A6ACCD;\">        self.network = nn.Sequential(</span></div><div class='line'><span style=\"color: #A6ACCD;\">            nn.Linear(4, 256),</span></div><div class='line'><span style=\"color: #A6ACCD;\">            nn.ReLU(),</span></div><div class='line'><span style=\"color: #A6ACCD;\">            nn.Linear(256, 256),</span></div><div class='line'><span style=\"color: #A6ACCD;\">            nn.ReLU(),</span></div><div class='line'><span style=\"color: #A6ACCD;\">            nn.Linear(256, 2),</span></div><div class='line'><span style=\"color: #A6ACCD;\">        )</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    def forward(self, x):</span></div><div class='line'><span style=\"color: #A6ACCD;\">        return self.network(x)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">def linear_schedule(start_e, end_e, duration, t):</span></div><div class='line'><span style=\"color: #A6ACCD;\">    return start_e + (end_e - start_e) * min(t / duration, 1)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">def main():</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser = argparse.ArgumentParser()</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--seed&quot;, type=int, default=0,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;random seed of the experiment&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--batch_size&quot;, type=int, default=2048,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;the batch size of sample from the reply memory&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--gamma&quot;, type=float, default=0.99,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;discount factor&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--tau&quot;, type=float, default=0.1,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;the target network update rate&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--target_network_frequency&quot;, type=int, default=1_000,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;the frequency of target network update&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--start_e&quot;, type=float, default=1.0,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;starting epsilon&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--end_e&quot;, type=float, default=0.01,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;ending epsilon&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--exploration_fraction&quot;, type=float, default=0.5,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;the timesteps it takes to update the target network&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--learning_rate&quot;, type=float, default=1e-3,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;learning rate&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--total_timesteps&quot;, type=int, default=1_000_000,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;total number of timesteps&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--buffer_size&quot;, type=int, default=100_000,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;buffer size&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--learning_starts&quot;, type=int, default=10_000,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;timestep to start learning&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--train_frequency&quot;, type=int, default=10,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;the frequency of training&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    parser.add_argument(&quot;--device&quot;, type=str, default=&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;,</span></div><div class='line'><span style=\"color: #A6ACCD;\">                        help=&quot;Device to run the experiment on&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    args = parser.parse_args()</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    random.seed(args.seed)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    np.random.seed(args.seed)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    torch.manual_seed(args.seed)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    torch.backends.cudnn.deterministic = True</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    device = torch.device(args.device)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    print(f&quot;Using device: {device}&quot;)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    q_network = QNetwork().to(device)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    target_network = QNetwork().to(device)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    target_network.load_state_dict(q_network.state_dict())</span></div><div class='line'><span style=\"color: #A6ACCD;\">    target_network.eval()</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    cos_annealing_scheduler = CosineAnnealingLR(optimizer, T_max=args.total_timesteps, eta_min=args.learning_rate * 0.01)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    replay_buffer = ReplayBuffer(args.buffer_size)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    env = aerodrome.make(&quot;cartpole-v0&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    obs, info = env.reset()</span></div><div class='line'><span style=\"color: #A6ACCD;\">    obs = np.array(obs, dtype=np.float32)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    print(obs, info)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    episode_length = []</span></div><div class='line'><span style=\"color: #A6ACCD;\">    current_step = 0</span></div><div class='line'><span style=\"color: #A6ACCD;\">    for step in tqdm(range(args.total_timesteps)):</span></div><div class='line'><span style=\"color: #A6ACCD;\">        current_step += 1</span></div><div class='line'><span style=\"color: #A6ACCD;\">        epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, step)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        if random.random() &lt; epsilon:</span></div><div class='line'><span style=\"color: #A6ACCD;\">            action = random.randint(0, 1)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        else:</span></div><div class='line'><span style=\"color: #A6ACCD;\">            with torch.no_grad():</span></div><div class='line'><span style=\"color: #A6ACCD;\">                q_values = q_network(torch.from_numpy(obs).unsqueeze(0).to(device))</span></div><div class='line'><span style=\"color: #A6ACCD;\">                action = torch.argmax(q_values, dim=1).cpu().numpy().item()</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">        next_obs, reward, terminated, truncated, info = env.step(action)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        next_obs = np.array(next_obs, dtype=np.float32)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">        real_next_obs = deepcopy(next_obs)</span></div><div class='line'><span style=\"color: #A6ACCD;\">        replay_buffer.push(obs, action, reward, real_next_obs, terminated, truncated)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">        obs = next_obs</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">        if step &gt; args.learning_starts:</span></div><div class='line'><span style=\"color: #A6ACCD;\">            if step % args.train_frequency == 0:</span></div><div class='line'><span style=\"color: #A6ACCD;\">                batch = replay_buffer.sample(args.batch_size)</span></div><div class='line'><span style=\"color: #A6ACCD;\">                batch[&quot;obs&quot;] = torch.from_numpy(batch[&quot;obs&quot;]).to(device)</span></div><div class='line'><span style=\"color: #A6ACCD;\">                batch[&quot;act&quot;] = torch.from_numpy(batch[&quot;act&quot;]).to(device)</span></div><div class='line'><span style=\"color: #A6ACCD;\">                batch[&quot;reward&quot;] = torch.from_numpy(batch[&quot;reward&quot;]).to(device)</span></div><div class='line'><span style=\"color: #A6ACCD;\">                batch[&quot;next_obs&quot;] = torch.from_numpy(batch[&quot;next_obs&quot;]).to(device)</span></div><div class='line'><span style=\"color: #A6ACCD;\">                batch[&quot;terminated&quot;] = torch.from_numpy(batch[&quot;terminated&quot;]).to(device)</span></div><div class='line'><span style=\"color: #A6ACCD;\">                batch[&quot;truncated&quot;] = torch.from_numpy(batch[&quot;truncated&quot;]).to(device)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">                with torch.no_grad():</span></div><div class='line'><span style=\"color: #A6ACCD;\">                    target_max, _ = target_network(batch[&quot;next_obs&quot;]).max(dim=1, keepdim=True)</span></div><div class='line'><span style=\"color: #A6ACCD;\">                    td_target = batch[&quot;reward&quot;] + args.gamma * (1 - batch[&quot;terminated&quot;]) * target_max - 10.0 * batch[&quot;terminated&quot;]</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">                old_val = q_network(batch[&quot;obs&quot;])</span></div><div class='line'><span style=\"color: #A6ACCD;\">                old_val = old_val.gather(1, batch[&quot;act&quot;].long())</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">                loss = F.mse_loss(td_target, old_val)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">                optimizer.zero_grad()</span></div><div class='line'><span style=\"color: #A6ACCD;\">                loss.backward()</span></div><div class='line'><span style=\"color: #A6ACCD;\">                optimizer.step()</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">            if step % args.target_network_frequency == 0:</span></div><div class='line'><span style=\"color: #A6ACCD;\">                for target_param, param in zip(target_network.parameters(), q_network.parameters()):</span></div><div class='line'><span style=\"color: #A6ACCD;\">                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">        cos_annealing_scheduler.step()</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">        if terminated or truncated:</span></div><div class='line'><span style=\"color: #A6ACCD;\">            obs, info = env.reset()</span></div><div class='line'><span style=\"color: #A6ACCD;\">            obs = np.array(obs, dtype=np.float32)</span></div><div class='line'><span style=\"color: #A6ACCD;\">            episode_length.append((step, current_step))</span></div><div class='line'><span style=\"color: #A6ACCD;\">            current_step = 0</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">    episode_length = np.array(episode_length)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    fig, ax = plt.subplots()</span></div><div class='line'><span style=\"color: #A6ACCD;\">    ax.plot(episode_length[:, 0], episode_length[:, 1], alpha=0.5, c=&quot;skyblue&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    moving_average = np.convolve(episode_length[:, 1], np.ones(100) / 100, mode=&#39;valid&#39;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    ax.plot(episode_length[99:, 0], moving_average, c=&quot;royalblue&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    ax.set_xlabel(&quot;steps&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    ax.set_ylabel(&quot;cumulative reward&quot;)</span></div><div class='line'><span style=\"color: #A6ACCD;\">    plt.show()</span></div><div class='line'>&nbsp;</div><div class='line'><span style=\"color: #A6ACCD;\">if __name__ == &quot;__main__&quot;:</span></div><div class='line'><span style=\"color: #A6ACCD;\">    main()</span></div>","classes":"torchlight","styles":"background-color: #292D3E; --theme-selection-background: #00000080;"}}
